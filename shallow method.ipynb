{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers  import Embedding\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense\n",
    "from keras import layers\n",
    "from keras import preprocessing\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_pickle(\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add new Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F-measure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj=['JJ','JJR','JJS']\n",
    "noun=['NN','NNS','NNPS','NNP']\n",
    "verb=['VB','VBD','VBG','VBN','VBP','VBZ']\n",
    "adverb=['RB','RBR','RBS']\n",
    "article=['RP']\n",
    "prep=['IN']\n",
    "pronoun=['PRP','PRP$']\n",
    "other=['CC','DT','CD','EX','FW','LS','MD','PDT','POS','SYM','TO','UH','WDT','WP','WP$','WRB','.',',']\n",
    "dicttag={}\n",
    "for i in adj:\n",
    "    dicttag[i]='ADJ'\n",
    "for i in noun:\n",
    "    dicttag[i]='Noun'\n",
    "for i in verb:\n",
    "    dicttag[i]='Verb'\n",
    "for i in adverb:\n",
    "    dicttag[i]='ADVERB'\n",
    "for i in pronoun:\n",
    "    dicttag[i]='PRONOUN'\n",
    "for i in other:\n",
    "    dicttag[i]='OTHER'\n",
    "dicttag['IN']='PREP'\n",
    "dicttag['RP']='ART'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicttagsent={}\n",
    "dicttagsent['Noun']='n'\n",
    "dicttagsent['Verb']='v'\n",
    "dicttagsent['ADJ']='a'\n",
    "dicttagsent['ADVERB']='r'\n",
    "dicttagsent['PREP']='n'\n",
    "dicttagsent['ART']='n'\n",
    "dicttagsent['OTHER']='n'\n",
    "dicttagsent['PRONOUN']='n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency(text):\n",
    "    mytxt=nltk.word_tokenize(text)\n",
    "    tag=nltk.pos_tag(mytxt)\n",
    "    dictfreq={}\n",
    "    for i in ['ADJ','Noun','Verb','ADVERB','PRONOUN','PREP','ART','OTHER']:\n",
    "        dictfreq[i]=0\n",
    "    for i in tag:\n",
    "        if dicttag[i[1]] in dictfreq:\n",
    "            dictfreq[dicttag[i[1]]]=dictfreq[dicttag[i[1]]]+1\n",
    "        else:\n",
    "            dictfreq[dicttag[i[1]]]=1\n",
    "    return dictfreq\n",
    "def F(text):\n",
    "    dic=frequency(text)\n",
    "    f=0.5*(dic['Noun']+dic['ADJ']+dic['PREP']+dic['ART']-dic['PRONOUN']-dic['Verb']-dic['ADVERB']+100)\n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gender Preferential Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(text):\n",
    "    mytxt=nltk.word_tokenize(text)\n",
    "    freq=0\n",
    "    for i in mytxt:\n",
    "        if i[-4:]=='able':\n",
    "            freq=freq+1\n",
    "    return freq\n",
    "def f2(text):\n",
    "    mytxt=nltk.word_tokenize(text)\n",
    "    freq=0\n",
    "    for i in mytxt:\n",
    "        if i[-2:]=='al':\n",
    "            freq=freq+1\n",
    "    return freq\n",
    "def f3(text):\n",
    "    mytxt=nltk.word_tokenize(text)\n",
    "    freq=0\n",
    "    for i in mytxt:\n",
    "        if i[-3:]=='ful':\n",
    "            freq=freq+1\n",
    "    return freq\n",
    "def f4(text):\n",
    "    mytxt=nltk.word_tokenize(text)\n",
    "    freq=0\n",
    "    for i in mytxt:\n",
    "        if i[-4:]=='ible':\n",
    "            freq=freq+1\n",
    "    return freq\n",
    "def f5(text):\n",
    "    mytxt=nltk.word_tokenize(text)\n",
    "    freq=0\n",
    "    for i in mytxt:\n",
    "        if i[-2:]=='ic':\n",
    "            freq=freq+1\n",
    "    return freq\n",
    "def f6(text):\n",
    "    mytxt=nltk.word_tokenize(text)\n",
    "    freq=0\n",
    "    for i in mytxt:\n",
    "        if i[-3:]=='ive':\n",
    "            freq=freq+1\n",
    "    return freq\n",
    "def f7(text):\n",
    "    mytxt=nltk.word_tokenize(text)\n",
    "    freq=0\n",
    "    for i in mytxt:\n",
    "        if i[-4:]=='less':\n",
    "            freq=freq+1\n",
    "    return freq\n",
    "def f8(text):\n",
    "    mytxt=nltk.word_tokenize(text)\n",
    "    freq=0\n",
    "    for i in mytxt:\n",
    "        if i[-2:]=='ly':\n",
    "            freq=freq+1\n",
    "    return freq\n",
    "def f9(text):\n",
    "    mytxt=nltk.word_tokenize(text)\n",
    "    freq=0\n",
    "    for i in mytxt:\n",
    "        if i[-3:]=='ous':\n",
    "            freq=freq+1\n",
    "    return freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surface features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import sentiwordnet as swn\n",
    "def sentcount(text):\n",
    "    return len(sent_tokenize(text))\n",
    "def wordcount(text):\n",
    "    return len(word_tokenize(text))\n",
    "def characterscount(text):\n",
    "    a=word_tokenize(text)\n",
    "    l=0\n",
    "    for i in a:\n",
    "        l=l+len(i)\n",
    "    return l\n",
    "def alphabetcount(text):\n",
    "    a=word_tokenize(text)\n",
    "    l=0\n",
    "    for i in a:\n",
    "        for j in i:\n",
    "            if j.isalpha():\n",
    "                l=l+1\n",
    "    return l\n",
    "def digitscount(text):\n",
    "    a=word_tokenize(text)\n",
    "    l=0\n",
    "    for i in a:\n",
    "        for j in i:\n",
    "            if j.isdigit():\n",
    "                l=l+1\n",
    "    return l\n",
    "def specount(text):\n",
    "    a=word_tokenize(text)\n",
    "    l=0\n",
    "    L=['@','#', '$', '%', '&', '*', '-', '=', '+', '¿', '¡', '[', ']', '{', '}', '/']\n",
    "    for i in a:\n",
    "        for j in i:\n",
    "            if j in L:\n",
    "                l=l+1\n",
    "    return l\n",
    "def punctcount(text):\n",
    "    a=word_tokenize(text)\n",
    "    l=0\n",
    "    L=[',',';',':','!','?','.']\n",
    "    for i in a:\n",
    "        for j in i:\n",
    "            if j in L:\n",
    "                l=l+1\n",
    "    return l\n",
    "def shortcount(text):\n",
    "    a=word_tokenize(text)\n",
    "    l=0\n",
    "    for i in a:\n",
    "        if len(i)<5:\n",
    "            l=l+1\n",
    "    return l\n",
    "def avgcount(text):\n",
    "    a=word_tokenize(text)\n",
    "    L=[len(i) for i in a]\n",
    "    return sum(L)/len(L)\n",
    "def sentiscore(text):\n",
    "    pos=0\n",
    "    neg=0\n",
    "    l=0\n",
    "    token = nltk.word_tokenize(text)\n",
    "    tagged = nltk.pos_tag(token)\n",
    "    for i in tagged:\n",
    "        tag=dicttagsent[dicttag[i[1]]]\n",
    "        a=swn.senti_synsets(i[0], tag)\n",
    "        posscore=0\n",
    "        negscore=0\n",
    "        for synst in a:\n",
    "            posscore=posscore+synst.pos_score()\n",
    "            negscore=negscore+synst.neg_score()\n",
    "        if posscore>0 or negscore>0:\n",
    "            pos=pos+posscore\n",
    "            neg=neg+negscore\n",
    "            l=l+1\n",
    "    return pos/l,neg/l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import re\n",
    "\n",
    "def tokenize(s):\n",
    "    tokens = re.split(r\"[^0-9A-Za-z\\-'_]+\", s)\n",
    "    return tokens\n",
    "\n",
    "def get_yules(s):\n",
    "    \n",
    "    tokens = tokenize(s)\n",
    "    token_counter = collections.Counter(tok.upper() for tok in tokens)\n",
    "    m1 = sum(token_counter.values())\n",
    "    m2 = sum([freq ** 2 for freq in token_counter.values()])\n",
    "    i = (m1*m1) / (m2-m1)\n",
    "    k = 1/i * 10000\n",
    "    return k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS Sequence Pattern Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "femclass=['shopping','mom','cried','freaked','pink','cute','gosh','kisses','yummy','mommy','boyfriend','skirt','adorable','husband','hubby']\n",
    "malclass=['linux',  'microsoft',  'gaming',  'server',  'software',  'gb', 'programming',  'google',  'data',  'graphics',  'india',  'nations', 'democracy', 'users', 'economic']\n",
    "acroclass=['gm', 'gn',  'brb',  'rofl','afaik',  'lmao', 'lmfao', 'lol',  'tgif', 'omg', 'omfg', 'tc', 'ttyl', 'af', 'ama', 'bae', 'dm', 'dafuq', 'eli5', 'fml', 'ftfy', 'hifw', 'icymi', 'imho', 'irl', 'mrw', 'yolo', 'xoxo']\n",
    "chastclass=['sexual', 'celibacy', 'abstinence',  'virtue',  'monogamy',  'virginity', 'pureness',  'unchaste',  'fasting',  'frugality',  'self-restraint', 'abnegation',  'asceticism',  'avoidance',  'renunciation',  'self-control',  'sobriety',  'teetotalism',  'monasticism',  'austerity', 'abstain', 'diet', 'hungry ']\n",
    "predclass=['forecast', 'guess', 'prognosis',  'prophecy',  'augury',  'conjecture', 'divination',  'foretell',  'hunch',  'horoscope',  'omen',  'presage', 'prevision', 'zodiac', 'fortune-telling', 'surmising']\n",
    "envyclass=['hatred',  'malice',  'prejudice',  'rivalry',  'backbiting',  'coveting', 'grudge',  'heartburn', 'malevolence',  'opposition', 'green-eyed', 'invidious', 'resentful']\n",
    "rainclass=['deluge',  'drizzle',  'flood', 'hail',  'mist', 'monsoon',  'precipitation', 'rainfall',  'shower',  'sleet', 'stream',  'torrent',  'pour', 'spate',  'spit', 'sprinkle', 'wet-stuff']\n",
    "evilclass=['criminal',  'devil',  'felon',  'gangster',  'lawbreaker',  'murderer', 'psychopath', 'sinner', 'sociopath', 'troublemaker', 'villain']\n",
    "mathclass=['mathematics', 'algebra', 'arithmetic', 'geometry', 'trigonometry', 'calculus',  'math', 'addition',  'division', 'multiply',  'figures',  'pie', 'chart', 'numbers', 'subtraction', 'plot', 'graph', 'data']\n",
    "groomclass=['groom', 'bride', 'suitor', 'benedict', 'spouse', 'fiancé']\n",
    "ethiclass=['ethics', 'belief', 'conduct', 'convention', 'honesty', 'honor', 'mores', 'values', 'ethos', 'practice', 'principles', 'conscience']\n",
    "fearclass=['fainthearted',  'terrified',  'angst',  'anxiety',  'concern',  'despair', 'dread',  'horror',  'jitters',  'panic',  'unease',  'worry',  'agitation', 'aversion',  'awe',  'consternation',  'creeps',  'discomposure', 'disquietude', 'distress', 'fright', 'nightmare', 'phobia']\n",
    "disgclass=['antipathy', 'dislike',  'distaste',  'loathe', 'revulsion', 'abhorrence', 'abominate', 'detest', 'hateful', 'nausea', 'objection', 'repugnance', 'revolt', 'satiation', 'satiety', 'sickness', 'surfeit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freqclass(text):\n",
    "    dict={}\n",
    "    classes=['femclass','malclass','acroclass','chastclass','predclass','envyclass','rainclass','evilclass','mathclass','groomclass','ethiclass','fearclass','disgclass']\n",
    "    for i in classes:\n",
    "        dict[i]=0\n",
    "    a=word_tokenize(text)\n",
    "    for i in a:\n",
    "        if i in femclass:\n",
    "            dict['femclass']=dict['femclass']+1\n",
    "        elif i in malclass:\n",
    "            dict['malclass']=dict['malclass']+1\n",
    "        elif i in acroclass:\n",
    "            dict['acroclass']=dict['acroclass']+1\n",
    "        elif i in chastclass:\n",
    "            dict['chastclass']=dict['chastclass']+1\n",
    "        elif i in groomclass:\n",
    "            dict['groomclass']=dict['groomclass']+1\n",
    "        elif i in ethiclass:\n",
    "            dict['ethiclass']=dict['ethiclass']+1\n",
    "        elif i in rainclass:\n",
    "            dict['rainclass']=dict['rainclass']+1\n",
    "        elif i in evilclass:\n",
    "            dict['evilclass']=dict['evilclass']+1\n",
    "        elif i in envyclass:\n",
    "            dict['envyclass']=dict['envyclass']+1\n",
    "        elif i in fearclass:\n",
    "            dict['fearclass']=dict['fearclass']+1\n",
    "        elif i in disgclass:\n",
    "            dict['disgclass']=dict['disgclass']+1\n",
    "        elif i in predclass:\n",
    "            dict['predclass']=dict['predclass']+1\n",
    "        elif i in mathclass:\n",
    "            dict['mathclass']=dict['mathclass']+1\n",
    "    return dict\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = pd.read_pickle(\"Dataf1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ModelTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#Data=Data.drop('post',axis=1)\n",
    "y = data.gender.values\n",
    "X_train, X_test, y_train, y_test = train_test_split(Data, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train=np.zeros(y_train.shape)\n",
    "for i in range(len(y_train)):\n",
    "    if y_train[i]=='male':\n",
    "        Y_train[i]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/infres/tabarani/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "Y_test=np.zeros(y_test.shape)\n",
    "for i in range(len(y_test)):\n",
    "    if y_test[i]=='male':\n",
    "        Y_test[i]=1\n",
    "X_test['age']=X_test['age'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/infres/tabarani/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/infres/tabarani/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "X_train['age']=X_train['age'].astype(float)\n",
    "X_test['age']=X_test['age'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=20, nthread=None, objective='binary:logistic',\n",
       "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "       seed=None, silent=True, subsample=1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xgb=XGBClassifier(n_jobs=20)\n",
    "xgb.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ModelTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=xgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.604236081551175"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_pred,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
